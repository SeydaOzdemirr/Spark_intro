{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "completed-louisville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/seyda.ozdemir/.conda/envs/Downloads/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /Users/seyda.ozdemir/.conda/envs/Downloads/lib/python3.8/site-packages (from pyspark) (0.10.9.3)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/seyda.ozdemir/.conda/envs/Downloads/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effective-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instructional-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unnecessary-overview",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/31 08:02:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "handmade-guest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-seyda-2eozdemir:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f474c13d160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-pittsburgh",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "final-tuesday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read the Dataset\n",
    "spark.read.option('header','true').option('sep',';').csv('Kitap3.csv').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "significant-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('Kitap3.csv', header=True, sep=';', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "loving-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "associate-paris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the Datatypes of the Columns\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "linear-theme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is type of data\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-junction",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selecting Columns and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "written-format",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Departments', 'Salary', 'Experience']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expected-clarity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', Departments='Data Science', Salary=10000, Experience=10),\n",
       " Row(Name='Krish', Departments='IOT', Salary=5000, Experience=8),\n",
       " Row(Name='Mahesh', Departments='Big Data', Salary=4000, Experience=4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting index\n",
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prospective-reproduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "split-canberra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "average-variable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    Krish|\n",
      "|    Krish|\n",
      "|   Mahesh|\n",
      "|    Krish|\n",
      "|   Mahesh|\n",
      "|Sudhanshu|\n",
      "|Sudhanshu|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "|    Sunny|\n",
      "|   Harsha|\n",
      "|  Shubham|\n",
      "|   Mahesh|\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "little-tracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Salary: int]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "south-compression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     Name|Salary|\n",
      "+---------+------+\n",
      "|    Krish| 10000|\n",
      "|    Krish|  5000|\n",
      "|   Mahesh|  4000|\n",
      "|    Krish|  4000|\n",
      "|   Mahesh|  3000|\n",
      "|Sudhanshu| 20000|\n",
      "|Sudhanshu| 10000|\n",
      "|Sudhanshu|  5000|\n",
      "|    Sunny| 10000|\n",
      "|    Sunny|  2000|\n",
      "|   Harsha| 15000|\n",
      "|  Shubham|  null|\n",
      "|   Mahesh| 40000|\n",
      "|     null| 38000|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "virtual-night",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Name'] # can't use the show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "worthy-exercise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Departments', 'string'),\n",
       " ('Salary', 'int'),\n",
       " ('Experience', 'int')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-disney",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check Describe option similar to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "searching-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Name: string, Departments: string, Salary: string, Experience: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stable-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+------------------+------------------+\n",
      "|summary|  Name|Departments|            Salary|        Experience|\n",
      "+-------+------+-----------+------------------+------------------+\n",
      "|  count|    13|         13|                13|                10|\n",
      "|   mean|  null|       null| 12769.23076923077|               5.1|\n",
      "| stddev|  null|       null|12728.929296120747|3.9285281382896233|\n",
      "|    min|Harsha|   Big Data|              2000|                 1|\n",
      "|    max| Sunny|        IOT|             40000|                10|\n",
      "+-------+------+-----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-niger",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Adding Columns in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "charitable-windows",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Departments: string, Salary: int, Experience: int, Salary with tips: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.withColumn('Salary with tips', df_pyspark['Salary']+df_pyspark['Salary']*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "antique-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+----------------+\n",
      "|     Name| Departments|Salary|Experience|Salary with tips|\n",
      "+---------+------------+------+----------+----------------+\n",
      "|    Krish|Data Science| 10000|        10|         12000.0|\n",
      "|    Krish|         IOT|  5000|         8|          6000.0|\n",
      "|   Mahesh|    Big Data|  4000|         4|          4800.0|\n",
      "|    Krish|    Big Data|  4000|         3|          4800.0|\n",
      "|   Mahesh|Data Science|  3000|         1|          3600.0|\n",
      "|Sudhanshu|Data Science| 20000|         2|         24000.0|\n",
      "|Sudhanshu|         IOT| 10000|      null|         12000.0|\n",
      "|Sudhanshu|    Big Data|  5000|        10|          6000.0|\n",
      "|    Sunny|Data Science| 10000|      null|         12000.0|\n",
      "|    Sunny|    Big Data|  2000|      null|          2400.0|\n",
      "|   Harsha|         IOT| 15000|         1|         18000.0|\n",
      "|  Shubham|        null|  null|         2|            null|\n",
      "|   Mahesh|Data Science| 40000|      null|         48000.0|\n",
      "|     null|    Big Data| 38000|        10|         45600.0|\n",
      "+---------+------------+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Salary with tips', df_pyspark['Salary']+df_pyspark['Salary']*0.2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-stand",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "signal-laser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Departments: string, Salary: int, Experience: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.drop('Salary with tips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "temporal-bearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Salary with tips').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "conservative-administration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.drop('Salary with tips')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-halloween",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "official-ground",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "| New Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-sandwich",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pyspark Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4c29335-58f2-4b25-9b89-5bf9ad1de2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-horizon",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Drop the missing data\n",
    "\n",
    "You can use the .na functions for missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)\n",
    "    \n",
    "    * param how: 'any' or 'all'.\n",
    "    \n",
    "        If 'any', drop a row if it contains any nulls.\n",
    "        If 'all', drop a row only if all its values are null.\n",
    "    \n",
    "    * param thresh: int, default None\n",
    "    \n",
    "        If specified, drop rows that have less than `thresh` non-null values.\n",
    "        This overwrites the `how` parameter.\n",
    "        \n",
    "    * param subset: \n",
    "        optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "forbidden-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----------+\n",
      "| Departments|Salary|Experience|\n",
      "+------------+------+----------+\n",
      "|Data Science| 10000|        10|\n",
      "|         IOT|  5000|         8|\n",
      "|    Big Data|  4000|         4|\n",
      "|    Big Data|  4000|         3|\n",
      "|Data Science|  3000|         1|\n",
      "|Data Science| 20000|         2|\n",
      "|         IOT| 10000|      null|\n",
      "|    Big Data|  5000|        10|\n",
      "|Data Science| 10000|      null|\n",
      "|    Big Data|  2000|      null|\n",
      "|         IOT| 15000|         1|\n",
      "|        null|  null|         2|\n",
      "|Data Science| 40000|      null|\n",
      "|    Big Data| 38000|        10|\n",
      "+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-disposal",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dropping Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "blond-beatles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop nan rows\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "classified-assembly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If 'any', drop a row if it contains any nulls.\n",
    "#If 'all', drop a row only if all its values are null.\n",
    "\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "executed-cornwall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#treshold: If specified, drop rows that have less than `thresh` non-null values.\n",
    "df_pyspark.na.drop(how='any' ,thresh = 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "classified-klein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any' ,thresh = 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "vital-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Subset: select column and drop that column have null values\n",
    "df_pyspark.na.drop(how='any' ,subset = ['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-digest",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filling the Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a9fdda30-841c-4158-8d41-2a8d068d9981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------+----------+\n",
      "|          Name| Departments|Salary|Experience|\n",
      "+--------------+------------+------+----------+\n",
      "|         Krish|Data Science| 10000|        10|\n",
      "|         Krish|         IOT|  5000|         8|\n",
      "|        Mahesh|    Big Data|  4000|         4|\n",
      "|         Krish|    Big Data|  4000|         3|\n",
      "|        Mahesh|Data Science|  3000|         1|\n",
      "|     Sudhanshu|Data Science| 20000|         2|\n",
      "|     Sudhanshu|         IOT| 10000|      null|\n",
      "|     Sudhanshu|    Big Data|  5000|        10|\n",
      "|         Sunny|Data Science| 10000|      null|\n",
      "|         Sunny|    Big Data|  2000|      null|\n",
      "|        Harsha|         IOT| 15000|         1|\n",
      "|       Shubham|        null|  null|         2|\n",
      "|        Mahesh|Data Science| 40000|      null|\n",
      "|Missing Values|    Big Data| 38000|        10|\n",
      "+--------------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Values',['Name','Salary']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "conscious-cedar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|         0|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|         0|\n",
      "|    Sunny|    Big Data|  2000|         0|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|     0|         2|\n",
      "|   Mahesh|Data Science| 40000|         0|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(0,['Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "initial-disability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-mitchell",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handling Missing values by Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-field",
   "metadata": {},
   "source": [
    "### Filling null values between mean, median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-commissioner",
   "metadata": {},
   "source": [
    "\n",
    "Imputation estimator for completing missing values, using the mean, median or mode\n",
    "of the columns in which the missing values are located. The input columns should be of\n",
    "numeric type. Currently Imputer does not support categorical features and\n",
    "possibly creates incorrect values for a categorical feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ahead-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Experience','Salary']]\n",
    "    ).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "drawn-bottle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+------------------+--------------+\n",
      "|     Name| Departments|Salary|Experience|Experience_imputed|Salary_imputed|\n",
      "+---------+------------+------+----------+------------------+--------------+\n",
      "|    Krish|Data Science| 10000|        10|                10|         10000|\n",
      "|    Krish|         IOT|  5000|         8|                 8|          5000|\n",
      "|   Mahesh|    Big Data|  4000|         4|                 4|          4000|\n",
      "|    Krish|    Big Data|  4000|         3|                 3|          4000|\n",
      "|   Mahesh|Data Science|  3000|         1|                 1|          3000|\n",
      "|Sudhanshu|Data Science| 20000|         2|                 2|         20000|\n",
      "|Sudhanshu|         IOT| 10000|      null|                 5|         10000|\n",
      "|Sudhanshu|    Big Data|  5000|        10|                10|          5000|\n",
      "|    Sunny|Data Science| 10000|      null|                 5|         10000|\n",
      "|    Sunny|    Big Data|  2000|      null|                 5|          2000|\n",
      "|   Harsha|         IOT| 15000|         1|                 1|         15000|\n",
      "|  Shubham|        null|  null|         2|                 2|         12769|\n",
      "|   Mahesh|Data Science| 40000|      null|                 5|         40000|\n",
      "|     null|    Big Data| 38000|        10|                10|         38000|\n",
      "+---------+------------+------+----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "charming-weight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+------------------+--------------+\n",
      "|     Name| Departments|Salary|Experience|Experience_imputed|Salary_imputed|\n",
      "+---------+------------+------+----------+------------------+--------------+\n",
      "|    Krish|Data Science| 10000|        10|                10|         10000|\n",
      "|    Krish|         IOT|  5000|         8|                 8|          5000|\n",
      "|   Mahesh|    Big Data|  4000|         4|                 4|          4000|\n",
      "|    Krish|    Big Data|  4000|         3|                 3|          4000|\n",
      "|   Mahesh|Data Science|  3000|         1|                 1|          3000|\n",
      "|Sudhanshu|Data Science| 20000|         2|                 2|         20000|\n",
      "|Sudhanshu|         IOT| 10000|      null|                 3|         10000|\n",
      "|Sudhanshu|    Big Data|  5000|        10|                10|          5000|\n",
      "|    Sunny|Data Science| 10000|      null|                 3|         10000|\n",
      "|    Sunny|    Big Data|  2000|      null|                 3|          2000|\n",
      "|   Harsha|         IOT| 15000|         1|                 1|         15000|\n",
      "|  Shubham|        null|  null|         2|                 2|         10000|\n",
      "|   Mahesh|Data Science| 40000|      null|                 3|         40000|\n",
      "|     null|    Big Data| 38000|        10|                10|         38000|\n",
      "+---------+------------+------+----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols = ['Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Experience','Salary']]\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-medicaid",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pyspark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "hungarian-doctor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark= df_pyspark.na.drop()\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-sponsorship",
   "metadata": {},
   "source": [
    "## Filter Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "increasing-accountability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Salary of the people less than or equal to 20000\n",
    "df_pyspark.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "rough-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     Name|Experience|\n",
      "+---------+----------+\n",
      "|    Krish|        10|\n",
      "|    Krish|         8|\n",
      "|   Mahesh|         4|\n",
      "|    Krish|         3|\n",
      "|   Mahesh|         1|\n",
      "|Sudhanshu|         2|\n",
      "|Sudhanshu|        10|\n",
      "|   Harsha|         1|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"Salary<=20000\").select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "productive-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Salary']<=20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "indian-london",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary']<=20000) & \n",
    "                  (df_pyspark['Salary']>=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dressed-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary']<=20000) | \n",
    "                  (df_pyspark['Salary']>=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "driven-brief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(~(df_pyspark['Salary']<=5000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-cabin",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pyspark GroupBy And Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-scoop",
   "metadata": {},
   "source": [
    "## Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "built-quarterly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f474c131ee0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.groupby('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "shaped-mozambique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+\n",
      "|     Name|sum(Salary)|sum(Experience)|\n",
      "+---------+-----------+---------------+\n",
      "|Sudhanshu|      25000|             12|\n",
      "|    Krish|      19000|             21|\n",
      "|   Harsha|      15000|              1|\n",
      "|   Mahesh|       7000|              5|\n",
      "+---------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouped to find the max salary\n",
    "df_pyspark.groupby('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "experimental-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+\n",
      "|     Name|max(Salary)|max(Experience)|\n",
      "+---------+-----------+---------------+\n",
      "|Sudhanshu|      20000|             10|\n",
      "|    Krish|      10000|             10|\n",
      "|   Harsha|      15000|              1|\n",
      "|   Mahesh|       4000|              4|\n",
      "+---------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "neither-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+\n",
      "|     Name|min(Salary)|min(Experience)|\n",
      "+---------+-----------+---------------+\n",
      "|Sudhanshu|       5000|              2|\n",
      "|    Krish|       4000|              3|\n",
      "|   Harsha|      15000|              1|\n",
      "|   Mahesh|       3000|              1|\n",
      "+---------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "piano-spending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+---------------+\n",
      "|     Name|      avg(Salary)|avg(Experience)|\n",
      "+---------+-----------------+---------------+\n",
      "|Sudhanshu|          12500.0|            6.0|\n",
      "|    Krish|6333.333333333333|            7.0|\n",
      "|   Harsha|          15000.0|            1.0|\n",
      "|   Mahesh|           3500.0|            2.5|\n",
      "+---------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "needed-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------------+\n",
      "| Departments|sum(Salary)|sum(Experience)|\n",
      "+------------+-----------+---------------+\n",
      "|         IOT|      20000|              9|\n",
      "|    Big Data|      13000|             17|\n",
      "|Data Science|      33000|             13|\n",
      "+------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby Departments which gives max salary\n",
    "df_pyspark.groupby('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "designing-rubber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+-----------------+\n",
      "| Departments|      avg(Salary)|  avg(Experience)|\n",
      "+------------+-----------------+-----------------+\n",
      "|         IOT|          10000.0|              4.5|\n",
      "|    Big Data|4333.333333333333|5.666666666666667|\n",
      "|Data Science|          11000.0|4.333333333333333|\n",
      "+------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "danish-prague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    3|\n",
      "|Data Science|    3|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "inappropriate-warner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      66000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-tower",
   "metadata": {},
   "source": [
    "## Funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "competitive-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg,stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cleared-pepper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|count(DISTINCT Departments)|\n",
      "+---------------------------+\n",
      "|                          3|\n",
      "+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(countDistinct(\"Departments\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "spatial-contents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Distinct Departments|\n",
      "+--------------------+\n",
      "|                   3|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(countDistinct(\"Departments\").alias(\"Distinct Departments\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "smart-speech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(Salary)|\n",
      "+-----------+\n",
      "|     8250.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(avg('Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "trying-emperor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|stddev_samp(Salary)|\n",
      "+-------------------+\n",
      "|   6227.81777878209|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(stddev(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-throw",
   "metadata": {},
   "source": [
    "That is a lot of precision for digits! Let's use the format_number to fix that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "pediatric-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ruled-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = df_pyspark.select(stddev(\"Salary\").alias('std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "sunrise-pledge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             std|\n",
      "+----------------+\n",
      "|6227.81777878209|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_std.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "double-explosion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|format_number(std, 2)|\n",
      "+---------------------+\n",
      "|             6,227.82|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# format_number(\"col_name\",decimal places)\n",
    "df_std.select(format_number('std',2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-intervention",
   "metadata": {},
   "source": [
    "### Order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "charitable-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OrderBy\n",
    "# Ascending\n",
    "df_pyspark.orderBy(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "compressed-citation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descending call off the column itself.\n",
    "df_pyspark.orderBy(df_pyspark[\"Name\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-leonard",
   "metadata": {},
   "source": [
    "# Dates and Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "muslim-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# May take a little while on a local computer\n",
    "spark = SparkSession.builder.appName(\"dates\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "thorough-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"appl_stock.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "apart-order",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "split-israel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "severe-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number,dayofmonth,hour,dayofyear,month,year,weekofyear,date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "failing-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|               4|\n",
      "|               5|\n",
      "|               6|\n",
      "|               7|\n",
      "|               8|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(dayofmonth(df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "purple-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|hour(Date)|\n",
      "+----------+\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(hour(df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "negative-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dayofyear(Date)|\n",
      "+---------------+\n",
      "|              4|\n",
      "|              5|\n",
      "|              6|\n",
      "|              7|\n",
      "|              8|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(dayofyear(df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "alike-shelter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|month(Date)|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(month(df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "south-intelligence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|year(Date)|\n",
      "+----------+\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(year(df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "filled-equity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|Year|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|2010|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|2010|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|2010|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Year\",year(df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "patent-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|avg(Year)|        avg(Close)|\n",
      "+---------+------------------+\n",
      "|   2015.0|120.03999980555547|\n",
      "|   2013.0| 472.6348802857143|\n",
      "|   2014.0| 295.4023416507935|\n",
      "|   2012.0| 576.0497195640002|\n",
      "|   2016.0|104.60400786904763|\n",
      "|   2010.0| 259.8424600000002|\n",
      "|   2011.0|364.00432532142867|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.withColumn(\"Year\",year(df['Date']))\n",
    "newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "hydraulic-material",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|  Year|Mean Close|\n",
      "+------+----------+\n",
      "|2015.0|    120.04|\n",
      "|2013.0|    472.63|\n",
      "|2014.0|    295.40|\n",
      "|2012.0|    576.05|\n",
      "|2016.0|    104.60|\n",
      "|2010.0|    259.84|\n",
      "|2011.0|    364.00|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']]\n",
    "result = result.withColumnRenamed(\"avg(Year)\",\"Year\")\n",
    "result = result.select('Year',format_number('avg(Close)',2).alias(\"Mean Close\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-farmer",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Examples Of Pyspark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "allied-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ml').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "unlike-dress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips = spark.read.csv('tips.csv', header=True, inferSchema = True)\n",
    "tips.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-motel",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "connected-symposium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_bill: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "consecutive-plastic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-pendant",
   "metadata": {},
   "source": [
    "## Handling categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "sublime-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling categorical features\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "anonymous-guest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|\n",
      "+----------+----+------+------+---+------+----+-----------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        1.0|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|        0.0|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|        0.0|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|        1.0|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|        0.0|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|        1.0|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|        1.0|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|        0.0|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|        1.0|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|        0.0|\n",
      "+----------+----+------+------+---+------+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol = \"sex\", outputCol='sex_indexed')\n",
    "df = indexer.fit(tips).transform(tips)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "prerequisite-sunset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|smoker_indexed|day_indexed|time_indexed|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|         0.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|         0.0|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|         0.0|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|         0.0|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|         0.0|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|         0.0|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|        0.0|           0.0|        0.0|         0.0|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCols = [\"smoker\",\"day\",\"time\"], outputCols =[\"smoker_indexed\",\"day_indexed\",\"time_indexed\"])\n",
    "df = indexer.fit(df).transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "broke-auckland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_bill',\n",
       " 'tip',\n",
       " 'sex',\n",
       " 'smoker',\n",
       " 'day',\n",
       " 'time',\n",
       " 'size',\n",
       " 'sex_indexed',\n",
       " 'smoker_indexed',\n",
       " 'day_indexed',\n",
       " 'time_indexed']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "wicked-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureassembler = VectorAssembler(inputCols=['tip','size','sex_indexed', 'smoker_indexed','day_indexed', 'time_indexed'],\n",
    "                                   outputCol= \"Independent Features\")\n",
    "output = featureassembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "measured-spray",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+--------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|smoker_indexed|day_indexed|time_indexed|Independent Features|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+--------------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|         0.0|[1.01,2.0,1.0,0.0...|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|[1.66,3.0,0.0,0.0...|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|[3.5,3.0,0.0,0.0,...|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[3.31,2.0,0.0,0.0...|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|         0.0|[3.61,4.0,1.0,0.0...|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|[4.71,4.0,0.0,0.0...|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[2.0,2.0,0.0,0.0,...|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|[3.12,4.0,0.0,0.0...|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[1.96,2.0,0.0,0.0...|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[3.23,2.0,0.0,0.0...|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[1.71,2.0,0.0,0.0...|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|         0.0|[5.0,4.0,1.0,0.0,...|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[1.57,2.0,0.0,0.0...|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|[3.0,4.0,0.0,0.0,...|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|         0.0|[3.02,2.0,1.0,0.0...|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[3.92,2.0,0.0,0.0...|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|         0.0|[1.67,3.0,1.0,0.0...|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|[3.71,3.0,0.0,0.0...|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|         0.0|[3.5,3.0,1.0,0.0,...|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|        0.0|           0.0|        0.0|         0.0|(6,[0,1],[3.35,3.0])|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "heavy-snowboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Independent Features|\n",
      "+--------------------+\n",
      "|[1.01,2.0,1.0,0.0...|\n",
      "|[1.66,3.0,0.0,0.0...|\n",
      "|[3.5,3.0,0.0,0.0,...|\n",
      "|[3.31,2.0,0.0,0.0...|\n",
      "|[3.61,4.0,1.0,0.0...|\n",
      "|[4.71,4.0,0.0,0.0...|\n",
      "|[2.0,2.0,0.0,0.0,...|\n",
      "|[3.12,4.0,0.0,0.0...|\n",
      "|[1.96,2.0,0.0,0.0...|\n",
      "|[3.23,2.0,0.0,0.0...|\n",
      "|[1.71,2.0,0.0,0.0...|\n",
      "|[5.0,4.0,1.0,0.0,...|\n",
      "|[1.57,2.0,0.0,0.0...|\n",
      "|[3.0,4.0,0.0,0.0,...|\n",
      "|[3.02,2.0,1.0,0.0...|\n",
      "|[3.92,2.0,0.0,0.0...|\n",
      "|[1.67,3.0,1.0,0.0...|\n",
      "|[3.71,3.0,0.0,0.0...|\n",
      "|[3.5,3.0,1.0,0.0,...|\n",
      "|(6,[0,1],[3.35,3.0])|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select('Independent Features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "integrated-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data = output.select(\"Independent Features\", \"total_bill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "round-wright",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|Independent Features|total_bill|\n",
      "+--------------------+----------+\n",
      "|[1.01,2.0,1.0,0.0...|     16.99|\n",
      "|[1.66,3.0,0.0,0.0...|     10.34|\n",
      "|[3.5,3.0,0.0,0.0,...|     21.01|\n",
      "|[3.31,2.0,0.0,0.0...|     23.68|\n",
      "|[3.61,4.0,1.0,0.0...|     24.59|\n",
      "|[4.71,4.0,0.0,0.0...|     25.29|\n",
      "|[2.0,2.0,0.0,0.0,...|      8.77|\n",
      "|[3.12,4.0,0.0,0.0...|     26.88|\n",
      "|[1.96,2.0,0.0,0.0...|     15.04|\n",
      "|[3.23,2.0,0.0,0.0...|     14.78|\n",
      "|[1.71,2.0,0.0,0.0...|     10.27|\n",
      "|[5.0,4.0,1.0,0.0,...|     35.26|\n",
      "|[1.57,2.0,0.0,0.0...|     15.42|\n",
      "|[3.0,4.0,0.0,0.0,...|     18.43|\n",
      "|[3.02,2.0,1.0,0.0...|     14.83|\n",
      "|[3.92,2.0,0.0,0.0...|     21.58|\n",
      "|[1.67,3.0,1.0,0.0...|     10.33|\n",
      "|[3.71,3.0,0.0,0.0...|     16.29|\n",
      "|[3.5,3.0,1.0,0.0,...|     16.97|\n",
      "|(6,[0,1],[3.35,3.0])|     20.65|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "vocational-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/31 08:04:22 WARN Instrumentation: [fbd3c730] regParam is zero, which might cause numerical instability and overfitting.\n",
      "22/03/31 08:04:22 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/03/31 08:04:22 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/03/31 08:04:22 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "22/03/31 08:04:22 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#train test split\n",
    "train_data, test_data = finalized_data.randomSplit([0.75,0.25])\n",
    "regressor = LinearRegression(featuresCol = 'Independent Features', labelCol='total_bill')\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "elect-assignment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([2.9617, 4.1035, -0.9153, 2.3002, -0.6408, -0.0401])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "hindu-insurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48864279052237697"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "committed-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred_result = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "charitable-savannah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+\n",
      "|Independent Features|total_bill|        prediction|\n",
      "+--------------------+----------+------------------+\n",
      "|(6,[0,1],[1.25,2.0])|     10.07|12.397647946329439|\n",
      "|(6,[0,1],[1.47,2.0])|     10.77|13.049213665922537|\n",
      "|(6,[0,1],[1.75,2.0])|     17.82|13.878479127222844|\n",
      "|(6,[0,1],[2.72,2.0])|     13.28| 16.75129161815605|\n",
      "| (6,[0,1],[3.0,4.0])|     20.45|25.787484283029904|\n",
      "|(6,[0,1],[3.18,2.0])|     19.82| 18.11365630457798|\n",
      "|[1.0,1.0,1.0,1.0,...|      3.07| 8.938622935990164|\n",
      "|[1.5,2.0,0.0,1.0,...|     15.69|14.797424931045576|\n",
      "|[1.5,2.0,1.0,0.0,...|      8.35|10.901000544174412|\n",
      "|[1.64,2.0,0.0,1.0...|     15.36|15.852892537672426|\n",
      "|[1.66,3.0,0.0,0.0...|     10.34|17.074558240472115|\n",
      "|[1.8,2.0,1.0,0.0,...|     12.43|11.789499252710456|\n",
      "|[1.83,1.0,1.0,0.0...|     10.07| 7.774885521777286|\n",
      "|[1.96,2.0,0.0,0.0...|     15.04|13.859593347221379|\n",
      "|[2.0,2.0,0.0,0.0,...|     13.81| 13.97805984169285|\n",
      "|[2.0,2.0,0.0,1.0,...|     14.48|16.278256111938983|\n",
      "|[2.0,2.0,1.0,0.0,...|     11.38|12.381831725067817|\n",
      "|[2.01,2.0,1.0,1.0...|     12.74|14.711644618931816|\n",
      "|[2.05,3.0,0.0,0.0...|     28.55|18.229606561568964|\n",
      "|[2.09,2.0,0.0,1.0...|     15.01| 17.18564060047649|\n",
      "+--------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_result.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "distinct-crack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46475134477174473, 4.264672612129887, 34.99137053567226)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result.r2, pred_result.meanAbsoluteError, pred_result.meanSquaredError"
   ]
  },
  {
   "cell_type": "raw",
   "id": "relative-gnome",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-italy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
