{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "completed-louisville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/seyda.ozdemir/.conda/envs/Downloads/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /Users/seyda.ozdemir/.conda/envs/Downloads/lib/python3.8/site-packages (from pyspark) (0.10.9.3)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/seyda.ozdemir/.conda/envs/Downloads/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effective-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instructional-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unnecessary-overview",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/31 08:02:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "handmade-guest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-seyda-2eozdemir:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f474c13d160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-pittsburgh",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "final-tuesday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read the Dataset\n",
    "spark.read.option('header','true').option('sep',';').csv('Kitap3.csv').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "significant-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('Kitap3.csv', header=True, sep=';', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "loving-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "associate-paris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the Datatypes of the Columns\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "linear-theme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is type of data\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-junction",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selecting Columns and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "written-format",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Departments', 'Salary', 'Experience']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expected-clarity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', Departments='Data Science', Salary=10000, Experience=10),\n",
       " Row(Name='Krish', Departments='IOT', Salary=5000, Experience=8),\n",
       " Row(Name='Mahesh', Departments='Big Data', Salary=4000, Experience=4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting index\n",
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prospective-reproduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "split-canberra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "average-variable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    Krish|\n",
      "|    Krish|\n",
      "|   Mahesh|\n",
      "|    Krish|\n",
      "|   Mahesh|\n",
      "|Sudhanshu|\n",
      "|Sudhanshu|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "|    Sunny|\n",
      "|   Harsha|\n",
      "|  Shubham|\n",
      "|   Mahesh|\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "little-tracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Salary: int]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "south-compression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     Name|Salary|\n",
      "+---------+------+\n",
      "|    Krish| 10000|\n",
      "|    Krish|  5000|\n",
      "|   Mahesh|  4000|\n",
      "|    Krish|  4000|\n",
      "|   Mahesh|  3000|\n",
      "|Sudhanshu| 20000|\n",
      "|Sudhanshu| 10000|\n",
      "|Sudhanshu|  5000|\n",
      "|    Sunny| 10000|\n",
      "|    Sunny|  2000|\n",
      "|   Harsha| 15000|\n",
      "|  Shubham|  null|\n",
      "|   Mahesh| 40000|\n",
      "|     null| 38000|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "virtual-night",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Name'] # can't use the show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "worthy-exercise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Departments', 'string'),\n",
       " ('Salary', 'int'),\n",
       " ('Experience', 'int')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-disney",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check Describe option similar to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "searching-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Name: string, Departments: string, Salary: string, Experience: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stable-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+------------------+------------------+\n",
      "|summary|  Name|Departments|            Salary|        Experience|\n",
      "+-------+------+-----------+------------------+------------------+\n",
      "|  count|    13|         13|                13|                10|\n",
      "|   mean|  null|       null| 12769.23076923077|               5.1|\n",
      "| stddev|  null|       null|12728.929296120747|3.9285281382896233|\n",
      "|    min|Harsha|   Big Data|              2000|                 1|\n",
      "|    max| Sunny|        IOT|             40000|                10|\n",
      "+-------+------+-----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-niger",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Adding Columns in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "charitable-windows",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Departments: string, Salary: int, Experience: int, Salary with tips: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.withColumn('Salary with tips', df_pyspark['Salary']+df_pyspark['Salary']*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "antique-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+----------------+\n",
      "|     Name| Departments|Salary|Experience|Salary with tips|\n",
      "+---------+------------+------+----------+----------------+\n",
      "|    Krish|Data Science| 10000|        10|         12000.0|\n",
      "|    Krish|         IOT|  5000|         8|          6000.0|\n",
      "|   Mahesh|    Big Data|  4000|         4|          4800.0|\n",
      "|    Krish|    Big Data|  4000|         3|          4800.0|\n",
      "|   Mahesh|Data Science|  3000|         1|          3600.0|\n",
      "|Sudhanshu|Data Science| 20000|         2|         24000.0|\n",
      "|Sudhanshu|         IOT| 10000|      null|         12000.0|\n",
      "|Sudhanshu|    Big Data|  5000|        10|          6000.0|\n",
      "|    Sunny|Data Science| 10000|      null|         12000.0|\n",
      "|    Sunny|    Big Data|  2000|      null|          2400.0|\n",
      "|   Harsha|         IOT| 15000|         1|         18000.0|\n",
      "|  Shubham|        null|  null|         2|            null|\n",
      "|   Mahesh|Data Science| 40000|      null|         48000.0|\n",
      "|     null|    Big Data| 38000|        10|         45600.0|\n",
      "+---------+------------+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Salary with tips', df_pyspark['Salary']+df_pyspark['Salary']*0.2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-stand",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "signal-laser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Departments: string, Salary: int, Experience: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.drop('Salary with tips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "temporal-bearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Salary with tips').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "conservative-administration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.drop('Salary with tips')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-halloween",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "official-ground",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "| New Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-sandwich",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pyspark Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4c29335-58f2-4b25-9b89-5bf9ad1de2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-horizon",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Drop the missing data\n",
    "\n",
    "You can use the .na functions for missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)\n",
    "    \n",
    "    * param how: 'any' or 'all'.\n",
    "    \n",
    "        If 'any', drop a row if it contains any nulls.\n",
    "        If 'all', drop a row only if all its values are null.\n",
    "    \n",
    "    * param thresh: int, default None\n",
    "    \n",
    "        If specified, drop rows that have less than `thresh` non-null values.\n",
    "        This overwrites the `how` parameter.\n",
    "        \n",
    "    * param subset: \n",
    "        optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "forbidden-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----------+\n",
      "| Departments|Salary|Experience|\n",
      "+------------+------+----------+\n",
      "|Data Science| 10000|        10|\n",
      "|         IOT|  5000|         8|\n",
      "|    Big Data|  4000|         4|\n",
      "|    Big Data|  4000|         3|\n",
      "|Data Science|  3000|         1|\n",
      "|Data Science| 20000|         2|\n",
      "|         IOT| 10000|      null|\n",
      "|    Big Data|  5000|        10|\n",
      "|Data Science| 10000|      null|\n",
      "|    Big Data|  2000|      null|\n",
      "|         IOT| 15000|         1|\n",
      "|        null|  null|         2|\n",
      "|Data Science| 40000|      null|\n",
      "|    Big Data| 38000|        10|\n",
      "+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-disposal",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dropping Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "blond-beatles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop nan rows\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "classified-assembly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If 'any', drop a row if it contains any nulls.\n",
    "#If 'all', drop a row only if all its values are null.\n",
    "\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "executed-cornwall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#treshold: If specified, drop rows that have less than `thresh` non-null values.\n",
    "df_pyspark.na.drop(how='any' ,thresh = 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "classified-klein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|      null|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|      null|\n",
      "|    Sunny|    Big Data|  2000|      null|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|   Mahesh|Data Science| 40000|      null|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any' ,thresh = 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "vital-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|  null|         2|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Subset: select column and drop that column have null values\n",
    "df_pyspark.na.drop(how='any' ,subset = ['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-digest",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filling the Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a9fdda30-841c-4158-8d41-2a8d068d9981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------+----------+\n",
      "|          Name| Departments|Salary|Experience|\n",
      "+--------------+------------+------+----------+\n",
      "|         Krish|Data Science| 10000|        10|\n",
      "|         Krish|         IOT|  5000|         8|\n",
      "|        Mahesh|    Big Data|  4000|         4|\n",
      "|         Krish|    Big Data|  4000|         3|\n",
      "|        Mahesh|Data Science|  3000|         1|\n",
      "|     Sudhanshu|Data Science| 20000|         2|\n",
      "|     Sudhanshu|         IOT| 10000|      null|\n",
      "|     Sudhanshu|    Big Data|  5000|        10|\n",
      "|         Sunny|Data Science| 10000|      null|\n",
      "|         Sunny|    Big Data|  2000|      null|\n",
      "|        Harsha|         IOT| 15000|         1|\n",
      "|       Shubham|        null|  null|         2|\n",
      "|        Mahesh|Data Science| 40000|      null|\n",
      "|Missing Values|    Big Data| 38000|        10|\n",
      "+--------------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling the string columns\n",
    "df_pyspark.na.fill('Missing Values',['Name','Salary']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "conscious-cedar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|         IOT| 10000|         0|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|    Sunny|Data Science| 10000|         0|\n",
      "|    Sunny|    Big Data|  2000|         0|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|  Shubham|        null|     0|         2|\n",
      "|   Mahesh|Data Science| 40000|         0|\n",
      "|     null|    Big Data| 38000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling the integer columns\n",
    "df_pyspark.na.fill(0,['Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-mitchell",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handling Missing values by Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-field",
   "metadata": {},
   "source": [
    "### Filling null values between mean, median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-commissioner",
   "metadata": {},
   "source": [
    "\n",
    "Imputation estimator for completing missing values, using the mean, median or mode\n",
    "of the columns in which the missing values are located. The input columns should be of\n",
    "numeric type. Currently Imputer does not support categorical features and\n",
    "possibly creates incorrect values for a categorical feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ahead-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Experience','Salary']]\n",
    "    ).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "drawn-bottle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+------------------+--------------+\n",
      "|     Name| Departments|Salary|Experience|Experience_imputed|Salary_imputed|\n",
      "+---------+------------+------+----------+------------------+--------------+\n",
      "|    Krish|Data Science| 10000|        10|                10|         10000|\n",
      "|    Krish|         IOT|  5000|         8|                 8|          5000|\n",
      "|   Mahesh|    Big Data|  4000|         4|                 4|          4000|\n",
      "|    Krish|    Big Data|  4000|         3|                 3|          4000|\n",
      "|   Mahesh|Data Science|  3000|         1|                 1|          3000|\n",
      "|Sudhanshu|Data Science| 20000|         2|                 2|         20000|\n",
      "|Sudhanshu|         IOT| 10000|      null|                 5|         10000|\n",
      "|Sudhanshu|    Big Data|  5000|        10|                10|          5000|\n",
      "|    Sunny|Data Science| 10000|      null|                 5|         10000|\n",
      "|    Sunny|    Big Data|  2000|      null|                 5|          2000|\n",
      "|   Harsha|         IOT| 15000|         1|                 1|         15000|\n",
      "|  Shubham|        null|  null|         2|                 2|         12769|\n",
      "|   Mahesh|Data Science| 40000|      null|                 5|         40000|\n",
      "|     null|    Big Data| 38000|        10|                10|         38000|\n",
      "+---------+------------+------+----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "charming-weight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+------------------+--------------+\n",
      "|     Name| Departments|Salary|Experience|Experience_imputed|Salary_imputed|\n",
      "+---------+------------+------+----------+------------------+--------------+\n",
      "|    Krish|Data Science| 10000|        10|                10|         10000|\n",
      "|    Krish|         IOT|  5000|         8|                 8|          5000|\n",
      "|   Mahesh|    Big Data|  4000|         4|                 4|          4000|\n",
      "|    Krish|    Big Data|  4000|         3|                 3|          4000|\n",
      "|   Mahesh|Data Science|  3000|         1|                 1|          3000|\n",
      "|Sudhanshu|Data Science| 20000|         2|                 2|         20000|\n",
      "|Sudhanshu|         IOT| 10000|      null|                 3|         10000|\n",
      "|Sudhanshu|    Big Data|  5000|        10|                10|          5000|\n",
      "|    Sunny|Data Science| 10000|      null|                 3|         10000|\n",
      "|    Sunny|    Big Data|  2000|      null|                 3|          2000|\n",
      "|   Harsha|         IOT| 15000|         1|                 1|         15000|\n",
      "|  Shubham|        null|  null|         2|                 2|         10000|\n",
      "|   Mahesh|Data Science| 40000|      null|                 3|         40000|\n",
      "|     null|    Big Data| 38000|        10|                10|         38000|\n",
      "+---------+------------+------+----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols = ['Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Experience','Salary']]\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-medicaid",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pyspark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "hungarian-doctor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark= df_pyspark.na.drop()\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-sponsorship",
   "metadata": {},
   "source": [
    "## Filter Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "increasing-accountability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Salary of the people less than or equal to 20000\n",
    "df_pyspark.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "rough-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     Name|Experience|\n",
      "+---------+----------+\n",
      "|    Krish|        10|\n",
      "|    Krish|         8|\n",
      "|   Mahesh|         4|\n",
      "|    Krish|         3|\n",
      "|   Mahesh|         1|\n",
      "|Sudhanshu|         2|\n",
      "|Sudhanshu|        10|\n",
      "|   Harsha|         1|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"Salary<=20000\").select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "productive-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Salary']<=20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "indian-london",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary']<=20000) & \n",
    "                  (df_pyspark['Salary']>=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dressed-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary']<=20000) | \n",
    "                  (df_pyspark['Salary']>=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "driven-brief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(~(df_pyspark['Salary']<=5000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-cabin",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pyspark GroupBy And Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-scoop",
   "metadata": {},
   "source": [
    "## Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "built-quarterly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f474c131ee0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.groupby('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "shaped-mozambique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+\n",
      "|     Name|sum(Salary)|sum(Experience)|\n",
      "+---------+-----------+---------------+\n",
      "|Sudhanshu|      25000|             12|\n",
      "|    Krish|      19000|             21|\n",
      "|   Harsha|      15000|              1|\n",
      "|   Mahesh|       7000|              5|\n",
      "+---------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouped to find the max salary\n",
    "df_pyspark.groupby('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "experimental-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+\n",
      "|     Name|max(Salary)|max(Experience)|\n",
      "+---------+-----------+---------------+\n",
      "|Sudhanshu|      20000|             10|\n",
      "|    Krish|      10000|             10|\n",
      "|   Harsha|      15000|              1|\n",
      "|   Mahesh|       4000|              4|\n",
      "+---------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "neither-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+\n",
      "|     Name|min(Salary)|min(Experience)|\n",
      "+---------+-----------+---------------+\n",
      "|Sudhanshu|       5000|              2|\n",
      "|    Krish|       4000|              3|\n",
      "|   Harsha|      15000|              1|\n",
      "|   Mahesh|       3000|              1|\n",
      "+---------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "piano-spending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+---------------+\n",
      "|     Name|      avg(Salary)|avg(Experience)|\n",
      "+---------+-----------------+---------------+\n",
      "|Sudhanshu|          12500.0|            6.0|\n",
      "|    Krish|6333.333333333333|            7.0|\n",
      "|   Harsha|          15000.0|            1.0|\n",
      "|   Mahesh|           3500.0|            2.5|\n",
      "+---------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "needed-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------------+\n",
      "| Departments|sum(Salary)|sum(Experience)|\n",
      "+------------+-----------+---------------+\n",
      "|         IOT|      20000|              9|\n",
      "|    Big Data|      13000|             17|\n",
      "|Data Science|      33000|             13|\n",
      "+------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby Departments which gives max salary\n",
    "df_pyspark.groupby('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "designing-rubber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+-----------------+\n",
      "| Departments|      avg(Salary)|  avg(Experience)|\n",
      "+------------+-----------------+-----------------+\n",
      "|         IOT|          10000.0|              4.5|\n",
      "|    Big Data|4333.333333333333|5.666666666666667|\n",
      "|Data Science|          11000.0|4.333333333333333|\n",
      "+------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "danish-prague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    3|\n",
      "|Data Science|    3|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "inappropriate-warner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      66000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-tower",
   "metadata": {},
   "source": [
    "## Funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "competitive-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg,stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cleared-pepper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|count(DISTINCT Departments)|\n",
      "+---------------------------+\n",
      "|                          3|\n",
      "+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(countDistinct(\"Departments\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "spatial-contents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Distinct Departments|\n",
      "+--------------------+\n",
      "|                   3|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(countDistinct(\"Departments\").alias(\"Distinct Departments\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "smart-speech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(Salary)|\n",
      "+-----------+\n",
      "|     8250.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(avg('Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "trying-emperor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|stddev_samp(Salary)|\n",
      "+-------------------+\n",
      "|   6227.81777878209|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(stddev(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-throw",
   "metadata": {},
   "source": [
    "That is a lot of precision for digits! Let's use the format_number to fix that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "pediatric-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ruled-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = df_pyspark.select(stddev(\"Salary\").alias('std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "sunrise-pledge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             std|\n",
      "+----------------+\n",
      "|6227.81777878209|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_std.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "double-explosion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|format_number(std, 2)|\n",
      "+---------------------+\n",
      "|             6,227.82|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# format_number(\"col_name\",decimal places)\n",
    "df_std.select(format_number('std',2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-intervention",
   "metadata": {},
   "source": [
    "### Order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "charitable-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OrderBy\n",
    "# Ascending\n",
    "df_pyspark.orderBy(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "compressed-citation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+----------+\n",
      "|     Name| Departments|Salary|Experience|\n",
      "+---------+------------+------+----------+\n",
      "|Sudhanshu|Data Science| 20000|         2|\n",
      "|Sudhanshu|    Big Data|  5000|        10|\n",
      "|   Mahesh|    Big Data|  4000|         4|\n",
      "|   Mahesh|Data Science|  3000|         1|\n",
      "|    Krish|Data Science| 10000|        10|\n",
      "|    Krish|         IOT|  5000|         8|\n",
      "|    Krish|    Big Data|  4000|         3|\n",
      "|   Harsha|         IOT| 15000|         1|\n",
      "+---------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descending call off the column itself.\n",
    "df_pyspark.orderBy(df_pyspark[\"Name\"].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-italy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
